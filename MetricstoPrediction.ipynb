{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b76519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np \n",
    "import random\n",
    "from geonamescache import GeonamesCache\n",
    "import networkx as nx\n",
    "from scipy.sparse import csr_matrix\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "#####################\n",
    "\n",
    "# Define the directory paths for loading the data\n",
    "load_directory = r\"\"\n",
    "\n",
    "# Load the network_matrices\n",
    "network_matrices_load_path = os.path.join(load_directory, \"network_matrices.pkl\")\n",
    "with open(network_matrices_load_path, 'rb') as f:\n",
    "    network_matrices = pickle.load(f)\n",
    "\n",
    "# Load the network_metrics\n",
    "network_metrics_load_path = os.path.join(load_directory, \"network_metrics.pkl\")\n",
    "with open(network_metrics_load_path, 'rb') as f:\n",
    "    network_metrics = pickle.load(f)\n",
    "\n",
    "##################\n",
    "\n",
    "# Find and remove rows with NaN values in network_metrics\n",
    "filtered_network_metrics = []\n",
    "filtered_network_matrices = []\n",
    "\n",
    "for i, metrics in enumerate(network_metrics):\n",
    "    if not any(np.isnan(value) for value in metrics.values() if isinstance(value, (int, float))):\n",
    "        filtered_network_metrics.append(metrics)\n",
    "        filtered_network_matrices.append(network_matrices[i])\n",
    "\n",
    "# Update network_metrics and network_matrices\n",
    "network_metrics = filtered_network_metrics\n",
    "network_matrices = filtered_network_matrices\n",
    "\n",
    "# Print the updated number of elements\n",
    "print(\"Number of elements in network_metrics:\", len(network_metrics))\n",
    "print(\"Number of elements in network_matrices:\", len(network_matrices))\n",
    "\n",
    "###################\n",
    "def prepare_data(network_matrices, network_metrics):\n",
    "    # Find the maximum dimensions of the adjacency matrices\n",
    "    max_num_nodes = max(mat.shape[0] for mat in network_matrices)\n",
    "    max_num_edges = max(mat.shape[1] for mat in network_matrices)\n",
    "\n",
    "    # Prepare input data (network metrics)\n",
    "    X = []\n",
    "    for metrics in network_metrics:\n",
    "        x = [\n",
    "            metrics['Num Nodes'],\n",
    "            metrics['Avg Degree'],\n",
    "            metrics['Avg Betweenness Centrality'],\n",
    "            metrics['Density'],\n",
    "            metrics['Assortativity'],\n",
    "            metrics['Avg Shortest Path Length'],\n",
    "            metrics['Diameter'],\n",
    "            metrics['Avg Closeness']\n",
    "        ]\n",
    "        X.append(x)\n",
    "    X = np.array(X)\n",
    "    \n",
    "\n",
    "    # Prepare output data (adjacency matrices)\n",
    "    y = []\n",
    "    for mat in network_matrices:\n",
    "        pad_rows = max_num_nodes - mat.shape[0]\n",
    "        pad_cols = max_num_edges - mat.shape[1]\n",
    "        if pad_rows > 0 or pad_cols > 0:\n",
    "            padded_mat = np.pad(mat.toarray(), [(0, pad_rows), (0, pad_cols)], mode='constant', constant_values=0)\n",
    "        else:\n",
    "            padded_mat = mat.toarray()\n",
    "        y.append(padded_mat)\n",
    "    y = np.stack(y, axis=0)\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Call the prepare_data function\n",
    "X_train, X_test, y_train, y_test = prepare_data(network_matrices, network_metrics)\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "## Fit the scaler on the training data and transform both training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Check the shapes of the data arrays\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"Data preparation completed successfully!\")\n",
    "\n",
    "# Concatenate X_train_scaled and X_test_scaled\n",
    "X_scaled = np.concatenate((X_train, X_test), axis=0)\n",
    "X_scaled = pd.DataFrame(X_scaled)\n",
    "\n",
    "print(\"CSV file saved successfully!\")\n",
    "X_scaled.to_csv(r'') \n",
    "\n",
    "#########################\n",
    "\n",
    "X_scaled = X_scaled.values\n",
    "# Calculate statistics for each feature in X_scaled\n",
    "statistics = {\n",
    "    'Feature': ['num_nodes', 'avg_degree', 'avg_betweenness_centrality', 'density', 'assortativity', 'average_shortest_path_length', 'diameter', 'avg_closeness'],\n",
    "    'Mean': [],\n",
    "    'Median': [],\n",
    "    'Mode': [],\n",
    "    'Standard Deviation': [],\n",
    "    'Mean + Std': [],\n",
    "    'Mean - Std': [],\n",
    "    'Variance': []\n",
    "}\n",
    "\n",
    "for i in range(X_scaled.shape[1]):\n",
    "    feature = X_scaled[:, i]\n",
    "    feature_mean = np.mean(feature)\n",
    "    feature_median = np.median(feature)\n",
    "    feature_mode = stats.mode(feature)[0][0]\n",
    "    feature_std = np.std(feature)\n",
    "    feature_std_plus_mean = feature_mean + feature_std \n",
    "    feature_std_minus_mean = feature_mean - feature_std \n",
    "    feature_var = np.var(feature)\n",
    "\n",
    "    statistics['Mean'].append(feature_mean)\n",
    "    statistics['Median'].append(feature_median)\n",
    "    statistics['Mode'].append(feature_mode)\n",
    "    statistics['Standard Deviation'].append(feature_std)\n",
    "    statistics['Mean + Std' ].append(feature_std_plus_mean)\n",
    "    statistics['Mean - Std'].append(feature_std_minus_mean)\n",
    "    statistics['Variance'].append(feature_var)\n",
    "\n",
    "# Create a DataFrame from the statistics dictionary\n",
    "df = pd.DataFrame(statistics)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "save_path = r''\n",
    "df.to_csv(save_path, index=False)\n",
    "print(f\"Statistics saved as CSV: {save_path}\")\n",
    "\n",
    "######################\n",
    "\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the graph neural network model\n",
    "class GraphNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GraphNeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()  # Add ReLU activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        ##### construct the structure of your model\n",
    "        \n",
    "        #Initialize the weights using Xavier initialization\n",
    "        init.xavier_uniform_(self.fc1.weight)\n",
    "        init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)  # Apply ReLU activation\n",
    "        x = self.fc2(x)\n",
    "        ##### construct the structure of your model\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Set the hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = \n",
    "output_size = y_train.shape[1] * y_train.shape[2]\n",
    "learning_rate = \n",
    "num_epochs = \n",
    "batch_size = \n",
    "\n",
    "# Create an instance of the graph neural network\n",
    "net = GraphNeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "####################\n",
    "\n",
    "# Mini-batch training\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the training data\n",
    "    indices = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_train_shuffled = X_train[indices]\n",
    "    y_train_shuffled = y_train[indices]\n",
    "\n",
    "    # Mini-batch training\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        # Extract mini-batch\n",
    "        X_batch = X_train_shuffled[i:i+batch_size]\n",
    "        y_batch = y_train_shuffled[i:i+batch_size]\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(X_batch)\n",
    "        outputs = outputs.view(-1, y_batch.shape[1], y_batch.shape[2])\n",
    "          # Reshape predicted output\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Display target and predicted output\n",
    "        target_output = y_batch[0].detach().numpy()\n",
    "        predicted_output = outputs[0].detach().numpy()\n",
    "        #print(f\"Target Output:\\n{target_output}\")\n",
    "        #print(f\"Predicted Output:\\n{predicted_output}\")\n",
    "        #print(\"-\" * 20)\n",
    "        \n",
    "\n",
    "    # Print the loss for every x epochs\n",
    "    if (epoch + 1) % x == 0:\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = r\"\"\n",
    "torch.save(net.state_dict(), model_save_path)\n",
    "\n",
    "#################\n",
    "# Evaluation\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    # Forward pass on the test set\n",
    "    outputs = net(X_test)\n",
    "    # Reshape the predicted outputs\n",
    "    outputs = outputs.view(outputs.shape[0], y_test.shape[1], y_test.shape[2])\n",
    "    # Compute the test loss\n",
    "    test_loss = criterion(outputs, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss.item()}\")\n",
    "\n",
    "# Print the predictions and ground truth for the first sample\n",
    "print(\"Predictions:\")\n",
    "print(outputs[0])\n",
    "print(\"Ground Truth:\")\n",
    "print(y_test[0])\n",
    "\n",
    "##############\n",
    "\n",
    "# Create an empty list to store the characteristics\n",
    "characteristics_list = []\n",
    "\n",
    "# Specify the directory to save the matrices\n",
    "matrices_dir = r''\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(matrices_dir, exist_ok=True)\n",
    "\n",
    "# Assuming you have the predict_adjacency_matrix function defined\n",
    "def predict_adjacency_matrix(model, city_metrics):\n",
    "    # Prepare the input data\n",
    "    X_new = prepare_input_data(city_metrics)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Predict the adjacency matrix using the model\n",
    "    with torch.no_grad():\n",
    "        predicted_adjacency_matrix = model(X_new)\n",
    "\n",
    "    return predicted_adjacency_matrix\n",
    "\n",
    "# Assuming you have the prepare_input_data function defined\n",
    "def prepare_input_data(city_metrics):\n",
    "    # Convert the city_metrics list to a dictionary with appropriate keys\n",
    "    city_metrics_dict = {\n",
    "        'Num Nodes': city_metrics[0],\n",
    "        'Avg Degree': city_metrics[1],\n",
    "        'Avg Betweenness Centrality': city_metrics[2],\n",
    "        'Density': city_metrics[3],\n",
    "        'Assortativity': city_metrics[4],\n",
    "        'Avg Shortest Path Length': city_metrics[5],\n",
    "        'Diameter': city_metrics[6],\n",
    "        'Avg Closeness': city_metrics[7]\n",
    "    }\n",
    "\n",
    "    # Convert the relevant metrics to a numpy array\n",
    "    X_new = torch.tensor([list(city_metrics_dict.values())], dtype=torch.float32)\n",
    "\n",
    "    return X_new\n",
    "\n",
    "# Assuming you have the net model defined\n",
    "#net = net  # Replace 'YourModel()' with your actual model instantiation\n",
    "\n",
    "# Iterate over all selected input values X_selected\n",
    "for i in range(num_rows_to_select):\n",
    "    # Get the city metrics for the current input value\n",
    "    city_metrics = X_selected[i].tolist()\n",
    "    new_city_num_nodes = y_train.shape[1]\n",
    "    city_metrics[0] = new_city_num_nodes\n",
    "\n",
    "    print(city_metrics)\n",
    "\n",
    "        # Predict the adjacency matrix for the new city\n",
    "    predicted_adjacency_matrix = predict_adjacency_matrix(net, city_metrics)\n",
    "    predicted_adjacency_matrix = predicted_adjacency_matrix.reshape(new_city_num_nodes, new_city_num_nodes)\n",
    "\n",
    "    # Normalize the predicted adjacency matrix between 0 and 1 using min-max normalization\n",
    "    normalized_adjacency_matrix = (predicted_adjacency_matrix - torch.min(predicted_adjacency_matrix)) / (\n",
    "            torch.max(predicted_adjacency_matrix) - torch.min(predicted_adjacency_matrix))\n",
    "\n",
    "    # Select a cutoff point based on your desired criteria\n",
    "    cutoff = x # Adjust this value as needed\n",
    "\n",
    "    # Convert the normalized adjacency matrix to a binary adjacency matrix based on the cutoff\n",
    "    binary_adjacency_matrix = torch.where(normalized_adjacency_matrix > cutoff, 1, 0)\n",
    "\n",
    "    # Convert the binary adjacency matrix to a numpy array for further processing (if needed)\n",
    "    binary_adjacency_matrix = binary_adjacency_matrix.numpy()\n",
    "\n",
    "    # Check if the row has all zeros after modification\n",
    "    if sum(binary_adjacency_matrix[i]) == 0:\n",
    "        binary_adjacency_matrix = np.delete(binary_adjacency_matrix, i, axis=0)\n",
    "        binary_adjacency_matrix = np.delete(binary_adjacency_matrix, i, axis=1)\n",
    "        new_city_num_nodes -= 1\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "    # Check rows and columns with all zeros\n",
    "    rows_to_remove = []\n",
    "    for i in range(new_city_num_nodes):\n",
    "        if all(value == 0 for value in binary_adjacency_matrix[i]) and all(\n",
    "                value == 0 for value in binary_adjacency_matrix[:, i]):\n",
    "            rows_to_remove.append(i)\n",
    "\n",
    "    # Remove rows and corresponding columns\n",
    "    binary_adjacency_matrix = np.delete(binary_adjacency_matrix, rows_to_remove, axis=0)\n",
    "    binary_adjacency_matrix = np.delete(binary_adjacency_matrix, rows_to_remove, axis=1)\n",
    "    \n",
    "    # Update the value of new_city_num_nodes\n",
    "    new_city_num_nodes = binary_adjacency_matrix.shape[0]\n",
    "\n",
    "    # Convert the modified binary adjacency matrix to a NetworkX graph\n",
    "    graph = nx.from_numpy_array(binary_adjacency_matrix, create_using=nx.Graph)\n",
    "\n",
    "    # Calculate the network characteristics\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    avg_degree = sum(dict(graph.degree()).values()) / num_nodes\n",
    "    betweenness_centrality = nx.betweenness_centrality(graph)\n",
    "    avg_betweenness_centrality = sum(betweenness_centrality.values()) / num_nodes\n",
    "    density = nx.density(graph)\n",
    "    assortativity = nx.degree_assortativity_coefficient(graph)\n",
    "\n",
    "    if nx.is_connected(graph):\n",
    "    # If the graph is connected, calculate the metrics directly\n",
    "        average_shortest_path_length = nx.average_shortest_path_length(graph)\n",
    "        diameter = nx.diameter(graph)\n",
    "        avg_closeness = sum(nx.closeness_centrality(graph).values()) / num_nodes\n",
    "    else:\n",
    "        # If the graph is disconnected, find the largest connected component\n",
    "        largest_component = max(nx.connected_components(graph), key=len)\n",
    "        subgraph = graph.subgraph(largest_component)\n",
    "\n",
    "        # Calculate the metrics for the largest connected component\n",
    "        average_shortest_path_length = nx.average_shortest_path_length(subgraph)\n",
    "        diameter = nx.diameter(subgraph)\n",
    "        avg_closeness = sum(nx.closeness_centrality(subgraph).values()) / len(largest_component)\n",
    "\n",
    "    # Add the characteristics to the list\n",
    "    characteristics_list.append({'Variation': variation, 'Number of nodes': num_nodes,\n",
    "                                 'Average degree': avg_degree,\n",
    "                                 'Average betweenness centrality': avg_betweenness_centrality,\n",
    "                                 'Density': density, 'Assortativity': assortativity,\n",
    "                                 'Average shortest path length': average_shortest_path_length,\n",
    "                                 'Diameter': diameter, 'Average closeness': avg_closeness})\n",
    "\n",
    "    # Save the predicted adjacency matrix as a CSV file\n",
    "    predicted_matrix_file_path = os.path.join(matrices_dir, f'predicted_{variation}.csv')\n",
    "    np.savetxt(predicted_matrix_file_path, predicted_adjacency_matrix, delimiter=',')\n",
    "    print(f\"Predicted adjacency matrix saved: {predicted_matrix_file_path}\")\n",
    "\n",
    "    # Save the binary adjacency matrix as a CSV file\n",
    "    binary_matrix_file_path = os.path.join(matrices_dir, f'binary_{variation}.csv')\n",
    "    np.savetxt(binary_matrix_file_path, binary_adjacency_matrix, delimiter=',')\n",
    "    print(f\"Binary adjacency matrix saved: {binary_matrix_file_path}\")\n",
    "\n",
    "# Create the DataFrame from the characteristics list\n",
    "characteristics_df = pd.DataFrame(characteristics_list)\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "characteristics_file_path = os.path.join(matrices_dir, 'characteristics.csv')\n",
    "characteristics_df.to_csv(characteristics_file_path, index=False)\n",
    "print(f\"Characteristics saved as CSV: {characteristics_file_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
